<!DOCTYPE html>
<html lang="en-US" prefix="og: http://ogp.me/ns#">

    <head>
        <title>iBreak</title>
        <style type="text/css">
            html, body {
                margin: 0;
                padding: 0;
                font-size: 13px;
                font-family: Helvetica, Arial, sans-serif;
                line-height: 1;
            }
            table {
                border-collapse: collapse;
                border-spacing: none;
            }
            p {
                margin: 0;
            }
        </style>
        <!-- <link href="" rel="stylesheet"> -->

        <!--
        <meta name="twitter:card" content="summary" />
        <meta name="twitter:site" content="@glenchsite" />
        <meta name="twitter:creator" content="@glench" />
        <meta property="og:type" content="article" />
        <meta property="og:url" content="http://glench.com/DeepListeningAtTheRecurseCenter/" />
        <meta property="og:title" content="Deep Listening at the Recurse Center" />
        <meta property="og:description" content="Helping programmers become more self-directed through meditative listening." />
        <meta property="og:image" content="http://glench.com/DeepListeningAtTheRecurseCenter/icon.png" />
        -->

    </head>
    <body>

        <video onloadedmetadata="onPlay(this)" id="inputVideo" autoplay muted></video>
        <canvas id="display" style="position: absolute;"></canvas>
        <h1></h1>

        <script src="face-api.js"></script>
        <script type="text/javascript">
// lots of code borrowed from https://github.com/justadudewhohacks/face-api.js/blob/master/examples/examples-browser/views/webcamFaceLandmarkDetection.html

const videoEl = document.querySelector('video')
const canvasEl = document.querySelector('canvas')
canvasEl.style.left = 0

async function load() {
    await faceapi.nets.tinyFaceDetector.loadFromUri('models/') // load face detector model data
    await faceapi.nets.faceLandmark68Net.loadFromUri('models/') // load facial landmarks model data

    const stream = await navigator.mediaDevices.getUserMedia({ video: {} }) // get webcam video data
    videoEl.srcObject = stream
}
load()

const faceOptions = new faceapi.TinyFaceDetectorOptions();

// 68 facial points reference: https://www.pyimagesearch.com/wp-content/uploads/2017/04/facial_landmarks_68markup-768x619.jpg
const TOP_LEFT_EYE_INDEX = 38 // 39 in 1-based array
const TOP_RIGHT_EYE_INDEX = 43 // 44 in 1-based array
const TOP_MOUTH_INDEX = 51 // 52 in 1-based array

function distance(point1, point2) {
    return Math.sqrt(Math.pow())
}

async function onPlay() {
    if (videoEl.paused || videoEl.ended || !faceapi.nets.tinyFaceDetector.isLoaded) {
        return requestAnimationFrame(onPlay)
    }
    const detection = await faceapi.detectSingleFace(videoEl, faceOptions).withFaceLandmarks()
    if (detection) {
        const dims = faceapi.matchDimensions(canvasEl, videoEl, true)
        const resizedResult = faceapi.resizeResults(detection, dims)
        faceapi.draw.drawDetections(canvasEl, resizedResult)
        faceapi.draw.drawFaceLandmarks(canvasEl, resizedResult)

        const left_eye = detection.landmarks.positions[TOP_LEFT_EYE_INDEX]
        const right_eye = detection.landmarks.positions[TOP_RIGHT_EYE_INDEX]
        const mid_eye = faceapi.getCenterPoint([left_eye, right_eye]) // right between the eyes
        const mouth = detection.landmarks.positions[TOP_MOUTH_INDEX]

        const face_height_in_camera = mid_eye.sub(mouth).magnitude() // measured on midline from midway between eyes to top lip
        document.querySelector('h1').innerText = 'Distance: '+face_height_in_camera
    }

    requestAnimationFrame(onPlay)
}




        </script>
    </body>
</html>
